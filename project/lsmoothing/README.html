<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>README</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">Laplacian Smoothing with Coarray Fortran</h1>

<p>For access to this homework on
github,
<a href="http://github.com/rehnd/cme342/tree/master/project">click here</a>.</p>

<h2 id="toc_1">0. Overview</h2>

<h3 id="toc_2">0.1 Coarray basics</h3>

<p>This project uses Fortran&#39;s 2008/2015 standards, which include
specifications for <em>coarray</em> Fortran. Coarrays allow code developers
to use state-of-the-art MPI constructs and parallelization without
ever having to call MPI subroutines or functions. </p>

<p>The purpose of coarray Fortran is to abstract away tedious MPI calls
by allowing users to specify distributed variables with an extra
dimension in square brackets <code>[*]</code>. This extra square bracket allows
users to access elements within a given <em>codimension</em>.  A simple
example is the following:</p>

<div><pre><code class="language-fortran">program coarray_test
integer              :: i, n = 100
real(8), allocatable :: x(:)[:]  ! coarray x

allocate(x(n)[*]) ! Allocates x to be of size n on each codimension

do i = 1,n
   x(i) = i*this_image()
end do

print *, &quot;Image &quot;, this_image(), &quot;of &quot;, num_images(), &quot; has x(1) = &quot;, x(1)

deallocate(x)

end program</code></pre></div>

<p>In this simple example, we create an allocatable coarray of size 100.
During runtime, the number of codimensions is determined by the number
of processes started up, just like in an MPI program. For example, we
would compile and then run the above code via</p>

<div><pre><code class="language-bash">$ caf -o coarray_test coarray_test.f90
$ cafrun -np 4 ./coarray_test</code></pre></div>

<p>Here, we use the <code>caf</code> compiler, which is essentially a wrapper around
<code>mpif90</code>, as well as <code>cafrun</code>, which is very similar to <code>mpirun</code>.</p>

<p>As can be seen in the code above, coarray code takes a significant
burden off of the programmer in terms of what needs to be done to
start up a parallel program. Specifically, we can see the following
constructs that are &quot;built-in&quot; in the example code:</p>

<ul>
<li>Specification of coarrays via <code>varname[*]</code></li>
<li><code>this_image()</code>: Returns the process/image id for each image</li>
<li><code>num_images()</code>: Returns the total number of images specified at runtime</li>
</ul>

<p>These intrinsic functions are very convenient to use, but this does
not mean that coarray code is in any way restrictive of advanced MPI
features. Coarrays naturally use advanced MPI features &#39;under the
hood&#39; which will be discussed later, but an important point is that
any coarray Fortran code can be modified to use any MPI functions or
subroutines that come built in to a particular MPI standard. For
example, it is perfectly valid to set up a new communicator in a
coarray code and program with standard MPI if desired. However,
coarrays are often a much better way to do things.</p>

<h3 id="toc_3">0.2 Purpose</h3>

<p>There are several points for my doing this project:</p>

<ol>
<li>Understand features of OpenCoarrays and how they compare to
standard MPI. The specific features of coarrays I learned about are:

<ul>
<li>General coarray variables</li>
<li>Concurrency loops, exploting concurrency</li>
<li>One-sided communication aspects of coarray Fortran</li>
<li>Intrinsic functions in Coarray Fortran</li>
<li>Installation of <code>opencoarrays</code></li>
</ul></li>
<li>Implement hw1 in Coarray Fortran to learn the following:

<ul>
<li>How easy is coarray Fortran to use?</li>
<li>How many lines of code are required to perform similar MPI tasks?</li>
<li>How does the performance compare?</li>
</ul></li>
</ol>

<p>In addressing these points, I will cover four topics, devoting a
section below to each topic:</p>

<ol>
<li>Concurrency loops</li>
<li>Runtime performance of MPI code vs. Coarray code</li>
<li>Comparison of number of lines of code/implementation difficulty
between MPI and Coarray Fortran</li>
<li>Additional features of Coarray Fortran used (and not used) in this project</li>
</ol>

<h1 id="toc_4">1. Concurrency loops</h1>

<p>Coarray Fortran was specifically designed with the idea of
high-performance numerical computing in mind. A new construct called
<code>do concurrent</code> was added to the language, and this feature allows
users to gain added performance benefits when performing operations
that do not carry data dependencies. A simple example is adding two
vectors together. In the normal Fortran case, we would write</p>

<div><pre><code class="language-fortran">do i=1,n
   z(i) = x(i) + y(i)
end do</code></pre></div>

<p>In Coarray Fortran, we can easily convert this loop to:</p>

<div><pre><code class="language-fortran">do concurrrent(i=1:n)
   z(i) = x(i) + y(i)
end do</code></pre></div>

<p>By writing <code>do concurrent</code>, we specify to the compiler that there are
no data dependencies between <code>z(i)</code> and <code>z(j)</code> for <code>i</code> not equal to
<code>j</code>. This allows the compiler to optimize the loop for faster
performance. The particular form of optimization is completely up to
the compiler, but the two most important capabilities are: 1)
threading and 2) vectorization.  Currently in <code>gfortran</code>, the compiler
uses vectorization when using <code>do concurrent</code>. However, this syntax
could just as well enable threading under the hood, so that the <code>do
concurrent</code> loop above is essentially turned in to an OpenMP construct.</p>

<p>This optimization is the first example of how coarray Fortran provides
high performance with very easy-to-use syntax. In my project code, I
have loops that look as follows:</p>

<div><pre><code class="language-fortran">   ! Standard Fortran loop
   do j = 2, n2me-1 
      do i = 2, n1me-1
         a(i,j) = b(i,j) + epsilon*(                         &amp; 
               b(i-1,j+1) +          b(i,j+1) + b(i+1,j+1) + &amp; 
               b(i-1,j  ) - dble(8.)*b(i,j  ) + b(i+1,j  ) + &amp; 
               b(i-1,j-1) +          b(i,j-1) + b(i+1,j-1)) 
       end do
    end do </code></pre></div>

<p>After modifying this, I have:</p>

<div><pre><code class="language-fortran">   ! Coarray concurrency loop
   do concurrent(j = 2:n2me-1)
      do concurrent(i = 2:n1me-1)
         a(i,j) = b(i,j) + epsilon*(                         &amp; 
               b(i-1,j+1) +          b(i,j+1) + b(i+1,j+1) + &amp; 
               b(i-1,j  ) - dble(8.)*b(i,j  ) + b(i+1,j  ) + &amp; 
               b(i-1,j-1) +          b(i,j-1) + b(i+1,j-1)) 
       end do
    end do</code></pre></div>

<p>The runtime performance of these two codes is shown below, using 1,2,
and 4 processors.</p>

<p><img src="pics/doconcurrent.png" alt=""></p>

<p>The reason here for the lower runtime of <code>do concurrent</code> is that the
compiler optimized the loop using vectorization. The nice part about
allowing the compiler to do this is that the code is portable across a
variety of machines, and each machine can have its own set of rules
for how to best optimize a <code>do concurrent</code> loop. Therefore, one
machine might use vectorization while another might implement
threading, depending on how the compiler decides to optimize the
code. This allows for fine-grained optimization for a particular
machine or computer architecture, while not sacrificing generality in
the code itself.</p>

<h2 id="toc_5">2. Runtime performance of MPI Fortran and Coarray Fortran</h2>

<p>In the first homework assignment, we implemented a solution to the
Laplace smoothing algorithm using 3 different MPI constructs:</p>

<ol>
<li><code>MPI_Send</code> and <code>MPI_Recv</code></li>
<li><code>MPI_SendRecv</code></li>
<li><code>MPI_ISend</code> and <code>MPI_IRecv</code> (non-blocking)</li>
</ol>

<h3 id="toc_6">2.1 Coarray implementation details</h3>

<p>For this project, I implemented two different versions of Coarray
Fortran code. The first, referred to as <code>Coarrays</code>, does the
following:</p>

<ul>
<li>Allocate coarrays <code>a(:,:)[:]</code> and <code>b(:,:)[:]</code> of size <code>n1me =
n1/np1</code> x <code>n2me = n2/np2</code> for each process</li>
<li>Store edge data in contiguous arrays of size <code>n1me</code> or <code>n2me</code></li>
<li>Send the contiguous arrays using coarray send construct (more on
this later).</li>
</ul>

<p>The second version of coarray code is referred to as <code>Coarrays w/
Derived Datatypes</code>. This version does the following:</p>

<ul>
<li>Allocate coarrays <code>a(:,:)[:]</code> and <code>b(:,:)[:]</code> of size <code>n1me =
n1/np1 + 2</code> x <code>n2me = n2/np2 + 2</code> for each process. This provides
border cells that allow storage of the edge data within each array</li>
<li>Send the border data using coarray constructs that implement Derived
datatypes under the hood.</li>
</ul>

<p>This second formulation allows for two significant advantages: 1) The
code is particularly simple to write since no contiguous border arrays
are needed and 2) MPI Derived data types are used under the hood.</p>

<p>The MPI derived datatypes used under the hood require further
explanation. When sending data under this <code>Coarrays w/ Derived
Datatypes</code> version, we have the following simple code:</p>

<div><pre><code class="language-fortran">  subroutine send_edges()

    ! Get N and S data
    if (nid(2) &gt; 0)  b(n1me+2,:) = b(2,:)[nid(2)]
    if (nid(4) &gt; 0)  b(1,:)      = b(n1me+1,:)[nid(4)]

    sync all

    ! Get W and E data
    if (nid(3) &gt; 0)  b(:,n2me+2) = b(:,2)[nid(3)]
    if (nid(1) &gt; 0)  b(:,1)      = b(:,n2me+1)[nid(1)]

    sync all
  end subroutine send_edges</code></pre></div>

<p>Note that here <code>nid</code> is of size 4 and contains the neighbor id&#39;s of
all surrounding cells. If no cell exists to the left (<code>i=1</code>), bottom
(<code>i=2
</code>), right (<code>i=3</code>), or top (<code>i=4</code>), then <code>nid(i)</code> is set to -1.</p>

<p>In the above example, we first ask whether a neighbor to the South
(<code>nid(2)</code>) exists. If it does, we grab the first row of data from
<code>nid(2)</code>&#39;s array and place it in the last row of our id.  This is done
via the line</p>

<div><pre><code class="language-fortran">b(n1me+2,:) = b(2,:)[nid(2)]</code></pre></div>

<p>We can see exactly how simple coarray code is in this line. Under the
hood, this operation involves an <code>MPI_Get</code> call by one process,
requesting the second row of <code>b</code> on the process corresponding to
<code>nid(2)</code>.  Importantly, this is a <em>one-sided MPI communication</em> under
the hood. (Generally, one-sided communication is more efficient than
two-sided communication.)</p>

<p>The other aspect of the above line that is very nice is that we are
sending <em>non-contiguous</em> data. By grabbing the second <em>row</em> of <code>b</code> in
image <code>nid(2)</code>, we are requesting values that are located <code>n1me+2</code>
values away in memory. Fortunately, coarrays implements MPI Derived
data types under the hood so that this noncontiguous block can be
sent.  This is a very significant advantage, since we know that MPI
derived datatypes are much faster on modern computer architectures,
due to the fact that modern processors use Remote Direct Memory Access
(RDMA). </p>

<p>Overall, we see from this very simple line three great advantages of
Coarray Fortran:</p>

<ol>
<li>One-sided MPI communication</li>
<li>MPI Derived datatypes used under the hood</li>
<li>Extremely simple syntax, no MPI calls</li>
</ol>

<h3 id="toc_7">2.2 Performance comparison and analysis</h3>

<p>We can now ask how the performance of the three versions of MPI listed
above compare with the two versions of coarray code compare.  The Wall
time of the three codes is shown in the Figure below.</p>

<p><img src="pics/walltime.png" alt=""></p>

<p>Here, we see that between the 3 MPI implementations, the non-blocking
is the fastest, as we expect. The blocking routines are roughly the
same level of performance.</p>

<p>We see, however, that the Coarray Fortran code is faster than any of
the MPI implementations. </p>

<p>I also studied the speedup for these 5 codes on a larger problem (one
for which <code>n1 = 4096</code>, <code>n2 = 4096</code>, as opposed to <code>n1 = n2 =
1024</code>. These results are shown in the figure below.</p>

<p><img src="pics/speedup.png" alt=""></p>

<p>These can easily be turned into an efficiency plot:</p>

<p><img src="pics/efficiency.png" alt=""></p>

<p>Both the speedup and efficiency show roughly the same behavior for MPI
and Coarrays, meaning that there is nothing to lose by switching to
Coarray code. In most problems, we would actually expect coarray code
to outperform the MPI implementations in terms of speedup/efficiency.</p>

<h3 id="toc_8">2.3 Important note on runtimes</h3>

<p>As can be seen above, I was only able to run the code on 1, 2, and 4
processors. The reason for this was a problem with the MPICH
installation on the cluster I was using. Since I was unable to get
MPICH to work correctly with the <code>caf</code> compiler on the cluster, I was
not able to obtain results on 8, 16 and more processors. I hope to
have time to install MPICH correctly at some point.</p>

<p>Because the cluster was not working correctly, I was forced to use a
virtual machine on my laptop to run the code. My laptop has 4 cores,
but even so, the virtual machine may have caused things to run much
slower than should be expected on my laptop. </p>

<p><strong>Important Note:</strong> <em>These considerations should explain why the
efficiency and speedup is so poor in the above plots.</em></p>

<h2 id="toc_9">3. Comparison of number of lines of code/implementation difficulty</h2>

<p>Another important metric for comparing Coarray Fortran to standard MPI
Fortran is to look at the code complexity. As has already been
described, Coarray Fortran code has far simpler syntax than
traditional MPI Fortran. However, we can quantify this in a simpler
way by looking at the number of lines of code involved in each
case. The figure below presents the number of lines of code.</p>

<p><img src="pics/codelines.png" alt=""></p>

<p>In the above figure, we count the lines of code for MPI and Coarray
Fortran in the following ways:</p>

<ul>
<li><strong>MPI Fortran:</strong> Count the total number of lines that include
<code>MPI</code>. This is almost entirely MPI commands, such as <code>MPI_Recv</code>,
<code>MPI_Barrier</code>, etc. </li>
<li><p><strong>Coarray Fortran:</strong> Count the number of <code>sync all</code> statements and
any statements involving array assignment, for example:</p>

<div><pre><code class="language-fortran">  b(1,:) = b(n,:)[2]</code></pre></div></li>
</ul>

<p>As shown in the figure, Coarray Fortran requires many fewer lines of
code to perform essentially the same task. In fact, if we were to
implement MPI Derived Data types using standard MPI Fortran, we would
expect to have even more lines of MPI code than the 3 examples shown
above. In some sense, counting the number of lines of code is also a
<em>generous</em> comparison for MPI.  If we were to count the number of
characters used in MPI code vs. coarray code, the discrepacnies in the
above figure would be even greater. Furthermore, it is difficult to
quantify debug time from a simple character/line count alone. Often
times, MPI code can be extremely tedious to debug, and the time spent
debugging might not always scale linearly with the number of lines or
characters. In these respects, Coarray Fortran offers a great
advantage.</p>

<h2 id="toc_10">4.  Additional features of Coarray Fortran</h2>

<p>In addition to the Coarray features described above, there are several
features new to the Fortran 2015 standards that deserve mention. One
important addition is the ability to do collective communication
routines with coarrays.</p>

<p>The collective communication calls include:</p>

<ul>
<li><code>co_sum</code> (collective sum): sum the elements of elements of a coarray
on each image and store the result in a specified image</li>
<li><code>co_reduce</code> (collective reduction operation): similar to <code>co_sum</code>,
but for a generic operation to be applied to all elements of a
coarray.</li>
<li>More examples...</li>
</ul>

<p>Of the above, I ended up using only <code>co_sum</code> to compute the total norm
of the array. That syntax looks as follows:</p>

<div><pre><code class="language-fortran">norm = sum(a**2)

call co_sum(norm, result_image=1)

if (this_image() == 1) norm = sqrt(norm)</code></pre></div>

<p>which stores the computed norm on image 1 (note that image 1 in
coarray Fortran is the first image -- not image 0, as is the case in
MPI Fortran).</p>

<p>In addition to these features, Coarray Fortran now (very recently) has
the ability to detect failed images. This is very important for
exascale computing, where the probability of a node failing becomes
much larger, due to the sheer number of processors running at a given
time. In these cases, it is important for the code to adapt and
respond to the failed images. Coarray Fortran, when built with very
recent versions of MPICH, includes the intrinsic function</p>

<div><pre><code class="language-fortran">failed_images()</code></pre></div>

<p>as well as other features to assist with these scenarios. This is very
recent, and although I have not used this feature before, it should be
very useful for code developers as we approach exascale computing. </p>




</body>

</html>
